{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.test_env import EnvTest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EnvTest((5, 5, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Reward:  -0.1\n",
      "2\n",
      "Reward:  0.0\n",
      "0\n",
      "Reward:  -1.0\n",
      "3\n",
      "Reward:  -0.1\n",
      "3\n",
      "Reward:  -0.1\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(\"Reward: \", reward)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf2_linear import Linear\n",
    "import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EnvTest((5, 5, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run(env, policy, model, verbose=0):\n",
    "#     done = False\n",
    "#     curr_state = env.reset()\n",
    "#     total_reward = 0\n",
    "#     steps = 0\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     while not done:\n",
    "#         action, action_values = policy(curr_state)\n",
    "#         next_state, reward, done, info = env.step(action)\n",
    "#         loss = model.update(curr_state, action, reward, next_state, done)\n",
    "#         total_reward += reward\n",
    "#         total_loss += loss\n",
    "#         steps += 1\n",
    "        \n",
    "#         if verbose > 0:\n",
    "#             print(\"State: {}\".format(np.sum(curr_state)))\n",
    "#             print(\"Action Values: {} Action: {}\".format(action_values, action))\n",
    "#             print(\"Loss: {}\".format(loss))\n",
    "#             print(\"Reward: {}\".format(reward))\n",
    "#             print(\"\")\n",
    "\n",
    "#         curr_state = next_state\n",
    "        \n",
    "#     return total_reward, total_loss, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_run(env, policy, model, verbose=0):\n",
    "    done = False\n",
    "    curr_state = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, action_values = policy(curr_state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "        model.replay_fill(curr_state, action, reward, next_state, done)\n",
    "        loss = model.replay_update()\n",
    "        \n",
    "        total_reward += reward\n",
    "        total_loss += loss\n",
    "        steps += 1\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"State: {}\".format(np.sum(curr_state)))\n",
    "            print(\"Action Values: {} Action: {}\".format(action_values, action))\n",
    "            print(\"Target Values: {}\".format(model.target_net(model.preprocess(next_state))))\n",
    "            print(\"Loss: {}\".format(loss))\n",
    "            print(\"Reward: {}\".format(reward))\n",
    "            print(\"\")\n",
    "\n",
    "        curr_state = next_state\n",
    "        \n",
    "    return total_reward, total_loss/steps, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_len = 500\n",
    "model = Linear(replay_buffer_len=buffer_len)\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "# !rm -rf ./logs\n",
    "\n",
    "# log_dir = \"logs/experiment/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/debugging\"+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "Writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "update_steps = 0\n",
    "run_episodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Replay Buffer\n",
    "\n",
    "while model.replay_buffer.num_in_buffer < buffer_len:\n",
    "    done = False\n",
    "    curr_state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        action = np.random.randint(5)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        model.replay_fill(curr_state, action, reward, next_state, done)\n",
    "        curr_state = next_state\n",
    "\n",
    "print(model.replay_buffer.num_in_buffer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test Overfit\n",
    "# for i in range(episodes):\n",
    "#     update_steps += 1\n",
    "#     loss = model.replay_update()\n",
    "#     if i % 2 == 0:\n",
    "#         with Writer.as_default():\n",
    "#             tf.summary.scalar('Loss', loss,\n",
    "#                               step=update_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Run to get State to Track\n",
    "done = False\n",
    "curr_state = env.reset()\n",
    "track_states = [curr_state]\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    action = np.random.randint(5)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    popped = False\n",
    "    for s in track_states:\n",
    "        if np.all(s==next_state):\n",
    "            popped = True\n",
    "            \n",
    "    if not popped:\n",
    "        track_states.append(next_state)\n",
    "       \n",
    "print(len(track_states))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(episodes):\n",
    "    total_reward, train_loss, steps,  = replay_run(env, model.policy, model)\n",
    "    update_steps += steps\n",
    "    run_episodes += 1\n",
    "    \n",
    "    if i % 10 == 0:        \n",
    "        with Writer.as_default():\n",
    "            tf.summary.scalar('Avg Loss', train_loss,\n",
    "                              step=update_steps)\n",
    "                              \n",
    "    if i % 50 == 0:\n",
    "        total_test_reward, test_loss, steps = replay_run(env, model.greedy_policy, model)\n",
    "        update_steps += steps\n",
    "\n",
    "        action, action_values = model.greedy_policy(track_states[0])\n",
    "        q_state_0_a0 = action_values[0,0] \n",
    "        q_state_0_a1 = action_values[0,1] \n",
    "        q_state_0_a2 = action_values[0,2] \n",
    "        q_state_0_a3 = action_values[0,3] \n",
    "        q_state_0_a4 = action_values[0,4] \n",
    "        \n",
    "        with Writer.as_default():\n",
    "\n",
    "            tf.summary.scalar('Epsiode Reward', total_test_reward,\n",
    "                              step=update_steps)\n",
    "\n",
    "            tf.summary.scalar('Q(s0,a0)', q_state_0_a0, step=update_steps)            \n",
    "            tf.summary.scalar('Q(s0,a1)', q_state_0_a1, step=update_steps)  \n",
    "            tf.summary.scalar('Q(s0,a2)', q_state_0_a2, step=update_steps)  \n",
    "            tf.summary.scalar('Q(s0,a3)', q_state_0_a3, step=update_steps)  \n",
    "            tf.summary.scalar('Q(s0,a4)', q_state_0_a4, step=update_steps)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_reward, test_loss, steps = replay_run(env, model.greedy_policy, model, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
