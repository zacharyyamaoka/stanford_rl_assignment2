{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.test_env import EnvTest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EnvTest((5, 5, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf2_linear import Linear\n",
    "import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EnvTest((5, 5, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run(env, policy, model, verbose=0):\n",
    "#     done = False\n",
    "#     curr_state = env.reset()\n",
    "#     total_reward = 0\n",
    "#     steps = 0\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     while not done:\n",
    "#         action, action_values = policy(curr_state)\n",
    "#         next_state, reward, done, info = env.step(action)\n",
    "#         loss = model.update(curr_state, action, reward, next_state, done)\n",
    "#         total_reward += reward\n",
    "#         total_loss += loss\n",
    "#         steps += 1\n",
    "        \n",
    "#         if verbose > 0:\n",
    "#             print(\"State: {}\".format(np.sum(curr_state)))\n",
    "#             print(\"Action Values: {} Action: {}\".format(action_values, action))\n",
    "#             print(\"Loss: {}\".format(loss))\n",
    "#             print(\"Reward: {}\".format(reward))\n",
    "#             print(\"\")\n",
    "\n",
    "#         curr_state = next_state\n",
    "        \n",
    "#     return total_reward, total_loss, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_run(env, policy, model, verbose=0):\n",
    "    done = False\n",
    "    curr_state = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, action_values = policy(curr_state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "        model.replay_fill(curr_state, action, reward, next_state, done)\n",
    "        loss = model.replay_update()\n",
    "        \n",
    "        total_reward += reward\n",
    "        total_loss += loss\n",
    "        steps += 1\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"State: {}\".format(np.sum(curr_state)))\n",
    "            print(\"Action Values: {} Action: {}\".format(action_values, action))\n",
    "            print(\"Target Values: {}\".format(model.target_net(model.preprocess(next_state))))\n",
    "            print(\"Loss: {}\".format(loss))\n",
    "            print(\"Reward: {}\".format(reward))\n",
    "            print(\"\")\n",
    "\n",
    "        curr_state = next_state\n",
    "        \n",
    "    return total_reward, total_loss/steps, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_len = 500\n",
    "model = Linear(replay_buffer_len=buffer_len)\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "# !rm -rf ./logs\n",
    "\n",
    "# log_dir = \"logs/experiment/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/increase_exploration\"+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "Writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "update_steps = 0\n",
    "run_episodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# Fill Replay Buffer\n",
    "\n",
    "while model.replay_buffer.num_in_buffer < buffer_len:\n",
    "    done = False\n",
    "    curr_state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        action = np.random.randint(5)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        model.replay_fill(curr_state, action, reward, next_state, done)\n",
    "        curr_state = next_state\n",
    "\n",
    "print(model.replay_buffer.num_in_buffer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test Overfit\n",
    "# for i in range(episodes):\n",
    "#     update_steps += 1\n",
    "#     loss = model.replay_update()\n",
    "#     if i % 2 == 0:\n",
    "#         with Writer.as_default():\n",
    "#             tf.summary.scalar('Loss', loss,\n",
    "#                               step=update_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Random Run to get State to Track\n",
    "done = False\n",
    "curr_state = env.reset()\n",
    "track_states = [curr_state]\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    action = np.random.randint(5)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    popped = False\n",
    "    for s in track_states:\n",
    "        if np.all(s==next_state):\n",
    "            popped = True\n",
    "            \n",
    "    if not popped:\n",
    "        track_states.append(next_state)\n",
    "       \n",
    "print(len(track_states))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING TARGET WEIGHTS step:  250\n",
      "UPDATING TARGET WEIGHTS step:  500\n",
      "UPDATING TARGET WEIGHTS step:  750\n",
      "UPDATING TARGET WEIGHTS step:  1000\n",
      "UPDATING TARGET WEIGHTS step:  1250\n",
      "UPDATING TARGET WEIGHTS step:  1500\n",
      "UPDATING TARGET WEIGHTS step:  1750\n",
      "UPDATING TARGET WEIGHTS step:  2000\n",
      "UPDATING TARGET WEIGHTS step:  2250\n",
      "UPDATING TARGET WEIGHTS step:  2500\n",
      "UPDATING TARGET WEIGHTS step:  2750\n",
      "UPDATING TARGET WEIGHTS step:  3000\n",
      "UPDATING TARGET WEIGHTS step:  3250\n",
      "UPDATING TARGET WEIGHTS step:  3500\n",
      "UPDATING TARGET WEIGHTS step:  3750\n",
      "UPDATING TARGET WEIGHTS step:  4000\n",
      "UPDATING TARGET WEIGHTS step:  4250\n",
      "UPDATING TARGET WEIGHTS step:  4500\n",
      "UPDATING TARGET WEIGHTS step:  4750\n",
      "UPDATING EPSILON:  0.975 step:  5000\n",
      "UPDATING TARGET WEIGHTS step:  5000\n",
      "UPDATING TARGET WEIGHTS step:  5250\n",
      "UPDATING TARGET WEIGHTS step:  5500\n",
      "UPDATING TARGET WEIGHTS step:  5750\n",
      "UPDATING TARGET WEIGHTS step:  6000\n",
      "UPDATING TARGET WEIGHTS step:  6250\n",
      "UPDATING TARGET WEIGHTS step:  6500\n",
      "UPDATING TARGET WEIGHTS step:  6750\n",
      "UPDATING TARGET WEIGHTS step:  7000\n",
      "UPDATING TARGET WEIGHTS step:  7250\n",
      "UPDATING TARGET WEIGHTS step:  7500\n",
      "UPDATING TARGET WEIGHTS step:  7750\n",
      "UPDATING TARGET WEIGHTS step:  8000\n",
      "UPDATING TARGET WEIGHTS step:  8250\n",
      "UPDATING TARGET WEIGHTS step:  8500\n",
      "UPDATING TARGET WEIGHTS step:  8750\n",
      "UPDATING TARGET WEIGHTS step:  9000\n",
      "UPDATING TARGET WEIGHTS step:  9250\n",
      "UPDATING TARGET WEIGHTS step:  9500\n",
      "UPDATING TARGET WEIGHTS step:  9750\n",
      "UPDATING EPSILON:  0.95 step:  10000\n",
      "UPDATING TARGET WEIGHTS step:  10000\n",
      "UPDATING TARGET WEIGHTS step:  10250\n",
      "UPDATING TARGET WEIGHTS step:  10500\n",
      "UPDATING TARGET WEIGHTS step:  10750\n",
      "UPDATING TARGET WEIGHTS step:  11000\n",
      "UPDATING TARGET WEIGHTS step:  11250\n",
      "UPDATING TARGET WEIGHTS step:  11500\n",
      "UPDATING TARGET WEIGHTS step:  11750\n",
      "UPDATING TARGET WEIGHTS step:  12000\n",
      "UPDATING TARGET WEIGHTS step:  12250\n",
      "UPDATING TARGET WEIGHTS step:  12500\n",
      "UPDATING TARGET WEIGHTS step:  12750\n",
      "UPDATING TARGET WEIGHTS step:  13000\n",
      "UPDATING TARGET WEIGHTS step:  13250\n",
      "UPDATING TARGET WEIGHTS step:  13500\n",
      "UPDATING TARGET WEIGHTS step:  13750\n",
      "UPDATING TARGET WEIGHTS step:  14000\n",
      "UPDATING TARGET WEIGHTS step:  14250\n",
      "UPDATING TARGET WEIGHTS step:  14500\n",
      "UPDATING TARGET WEIGHTS step:  14750\n",
      "UPDATING EPSILON:  0.9249999999999999 step:  15000\n",
      "UPDATING TARGET WEIGHTS step:  15000\n",
      "UPDATING TARGET WEIGHTS step:  15250\n",
      "UPDATING TARGET WEIGHTS step:  15500\n",
      "UPDATING TARGET WEIGHTS step:  15750\n",
      "UPDATING TARGET WEIGHTS step:  16000\n",
      "UPDATING TARGET WEIGHTS step:  16250\n",
      "UPDATING TARGET WEIGHTS step:  16500\n",
      "UPDATING TARGET WEIGHTS step:  16750\n",
      "UPDATING TARGET WEIGHTS step:  17000\n",
      "UPDATING TARGET WEIGHTS step:  17250\n",
      "UPDATING TARGET WEIGHTS step:  17500\n",
      "UPDATING TARGET WEIGHTS step:  17750\n",
      "UPDATING TARGET WEIGHTS step:  18000\n",
      "UPDATING TARGET WEIGHTS step:  18250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-28247fd5de9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtotal_test_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mupdate_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-5f27823efbed>\u001b[0m in \u001b[0;36mreplay_run\u001b[0;34m(env, policy, model, verbose)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Projects/Deep RL Research/stanford_rl/Assignment 2 Assets/assignment2/tf2_linear.py\u001b[0m in \u001b[0;36mreplay_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_value\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         self.Optimizer.apply_gradients(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2_robo_research/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2_robo_research/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2_robo_research/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2_robo_research/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_GatherV2Grad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    538\u001b[0m       [[outer_dims], outer_axes_indices, inner_axes_indices], 0)\n\u001b[1;32m    539\u001b[0m   \u001b[0mvalues_transpose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m   \u001b[0mnum_segments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m   params_grad = math_ops.unsorted_segment_sum(values_transpose, indices,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2_robo_research/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    777\u001b[0m       \u001b[0m_check_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m       \u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m       \u001b[0mstrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m       \u001b[0mshrink_axis_mask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2_robo_research/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    910\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2_robo_research/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2_robo_research/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    529\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AddV2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         name, _ctx._post_execution_callbacks, x, y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(episodes):\n",
    "    total_reward, train_loss, steps,  = replay_run(env, model.policy, model)\n",
    "    update_steps += steps\n",
    "    run_episodes += 1\n",
    "    \n",
    "    if i % 10 == 0:        \n",
    "        with Writer.as_default():\n",
    "            tf.summary.scalar('Avg Loss', train_loss,\n",
    "                              step=update_steps)\n",
    "                              \n",
    "    if i % 50 == 0:\n",
    "        total_test_reward, test_loss, steps = replay_run(env, model.greedy_policy, model)\n",
    "        update_steps += steps\n",
    "\n",
    "        action, action_values = model.greedy_policy(track_states[0])\n",
    "        q_state_0_a0 = action_values[0,0] \n",
    "        q_state_0_a1 = action_values[0,1] \n",
    "        q_state_0_a2 = action_values[0,2] \n",
    "        q_state_0_a3 = action_values[0,3] \n",
    "        q_state_0_a4 = action_values[0,4] \n",
    "        \n",
    "        with Writer.as_default():\n",
    "\n",
    "            tf.summary.scalar('Epsiode Reward', total_test_reward,\n",
    "                              step=update_steps)\n",
    "\n",
    "            tf.summary.scalar('Q(s0,a0)', q_state_0_a0, step=update_steps)            \n",
    "            tf.summary.scalar('Q(s0,a1)', q_state_0_a1, step=update_steps)  \n",
    "            tf.summary.scalar('Q(s0,a2)', q_state_0_a2, step=update_steps)  \n",
    "            tf.summary.scalar('Q(s0,a3)', q_state_0_a3, step=update_steps)  \n",
    "            tf.summary.scalar('Q(s0,a4)', q_state_0_a4, step=update_steps)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 578\n",
      "Action Values: [[0.43660283 0.61546206 0.60590816 0.5479257  0.46870726]] Action: 1\n",
      "Loss: 0.1993013620376587\n",
      "Reward: -0.2\n",
      "\n",
      "State: 3080\n",
      "Action Values: [[0.4642187  0.62578124 0.57980835 0.5571435  0.44431806]] Action: 1\n",
      "Loss: 0.25547799468040466\n",
      "Reward: -0.2\n",
      "\n",
      "State: 3080\n",
      "Action Values: [[0.4638875  0.6284856  0.58280957 0.5583675  0.45008194]] Action: 1\n",
      "Loss: 0.15013673901557922\n",
      "Reward: -0.2\n",
      "\n",
      "State: 3080\n",
      "Action Values: [[0.45939642 0.62841815 0.58631504 0.55850184 0.45691758]] Action: 1\n",
      "Loss: 0.11135254055261612\n",
      "Reward: -0.2\n",
      "\n",
      "State: 3080\n",
      "Action Values: [[0.47078347 0.62605476 0.58181906 0.5564437  0.4635376 ]] Action: 1\n",
      "Loss: 0.12828384339809418\n",
      "Reward: -0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_test_reward, test_loss, steps = replay_run(env, model.greedy_policy, model, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
